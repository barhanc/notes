1. WHAT IS A NEURAL NETWORK?

Neural network is any Directed Acyclic Graph (DAG) in which every node i has the following
attributes.

1.  Set of previous nodes ğ’«áµ¢

2.  Set of next nodes ğ’©áµ¢

3.  Parametrized tensor function with signature 

    Fáµ¢ : â„^(...) Ã— ... Ã— â„^(...) Ã— Î˜ â†¦ â„^(...)
    
    The function takes p tensor arguments of dimensions kâ‚,...,kâ‚š respectively and parameters Î¸áµ¢ âˆˆ Î˜
    and returns a tensor of dimension l. Obviously it must satisfy p = |ğ’«áµ¢| and the tensor returned
    by parent nodes must have appropriate shapes.

4. The gradient of the function Fáµ¢ w.r.t. the parameters âˆ‚Fáµ¢[Î²]/âˆ‚Î¸áµ¢[Î±] and w.r.t. all the inputs
that is for all j âˆˆ ğ’«áµ¢ we have functions âˆ‚Fáµ¢[Î²]/âˆ‚Fâ±¼[Î±], where Î±, Î² denote some multi-indices.

---------------------------------------------------------------------------------------------------

2. LOSS FUNCTIONS

Training a neural network consists of changing the parameters Î¸_i of the nodes in such a way as to
make the network perform the given task. The task is specified by a training set ğ’³ which contains
"blueprint answers" of the network. To train the network we introduce the quantitative measure of
network's performance on the dataset which implicitly (through the network's outputs) depends on the
parameters Î¸ of the network L(ğ’³,Î¸). Training can then be phrased as an optimization problem of the
form

Î¸* = argmin L(ğ’³,Î¸)

for a fixed training set ğ’³.

There is no single established way of constructing loss functions. One of the more motivated
approaches is based on the maximum likelihood criterion. The idea is that we model our data using
some parametrized statistical model and express the parameters of this model as an output of a
neural network. The loss function is then taken to be the negated log-likelihood function. In this
manner one can derive the most common loss functions.

2.1. Mean Squared Error TODO:

2.2 (Binary) Cross Entropy TODO:

---------------------------------------------------------------------------------------------------

3. FORWARD PROPAGATION

Let váµ¢ be the (tensor) value of the function Fáµ¢. To propagate the (tensor) inputs through the
network we use the following recursive equation

váµ¢ = Fáµ¢( (vâ±¼ | j âˆˆ ğ’«áµ¢) ; Î¸áµ¢ )

and visit the nodes in the topological order as this guarantees that we visit every node exactly
once. We assume here that nodes váµ¢ such that ğ’«áµ¢ = âˆ… are the inputs of the network and nodes váµ¢ such
that ğ’©áµ¢ = âˆ… are the outputs of the network.

---------------------------------------------------------------------------------------------------

4. BACKWARD PROPAGATION

Let L be the loss function. In order to compute the derivatives âˆ‚L/âˆ‚Î¸áµ¢ we use the following
recursive equations

âˆ‚L/âˆ‚Î¸áµ¢[Î±] = âˆ‘{Î²} âˆ‚L/âˆ‚Fáµ¢[Î²] â‹… âˆ‚Fáµ¢[Î²]/âˆ‚Î¸áµ¢[Î±]

âˆ‚L/âˆ‚Fáµ¢[Î±] = âˆ‘{j âˆˆ ğ’©áµ¢} âˆ‘{Î²} âˆ‚L/âˆ‚Fâ±¼[Î²] â‹… âˆ‚Fâ±¼[Î²]/âˆ‚Fáµ¢[Î±]

where Î±, Î² are some multi-indices. We visit the nodes in the reversed topological order and compute
and store the values of loss function gradients. All gradients are computed for the current values
váµ¢ and Î¸áµ¢, therefore before backward propagation one must perform forward propagation to compute
values váµ¢.


háµ¢ = âˆ‘{Î²} X[Î±'Î²] â‹… Wáµ¢[Î²Î±] + báµ¢[Î±]

Fáµ¢[Î±'Î±](X ; Wáµ¢, báµ¢) = Ï•( âˆ‘{Î²} X[Î±'Î²] â‹… Wáµ¢[Î²Î±] + báµ¢[Î±])

âˆ‚Fáµ¢[Î±'Î±]/âˆ‚Wáµ¢[Î¼'Î¼] (X ; Wáµ¢, báµ¢) = Ï•'(háµ¢) â‹… âˆ‘{Î²} X[Î±'Î²] â‹… Î´[Î¼'Î²] â‹… Î´[Î¼Î±] = Ï•'(háµ¢) â‹… X[Î±'Î¼'] â‹… Î´[Î¼Î±]

âˆ‚Fáµ¢[Î±'Î±]/âˆ‚X[Î¼'Î¼] (X ; Wáµ¢, báµ¢) = Ï•'(háµ¢) â‹… âˆ‘{Î²} Wáµ¢[Î²Î±] â‹… Î´[Î¼'Î±'] â‹… Î´[Î¼Î²] = Ï•'(háµ¢) â‹… Wáµ¢[Î¼Î±] â‹… Î´[Î¼'Î±']
